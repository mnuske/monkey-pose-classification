{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet18, resnet50\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_HOME=models\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_HOME=models\n",
    "model = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input for pretrained models\n",
    "---\n",
    "\n",
    "Images have to have shape (3xHxW), H and W are expected to be at least 224.\n",
    "\n",
    "Images have to be loaded in to a range of [0, 1] and then normalized using the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforming_data = {\n",
    "    'train_data': transforms.Compose([\n",
    "                        transforms.RandomResizedCrop(224),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        # transforms.CenterCrop(224),\n",
    "                        # transforms.Resize((224, 224))\n",
    "                        transforms.ToTensor(),\n",
    "                        normalize,\n",
    "                    ]),\n",
    "    'validation_data': transforms.Compose([\n",
    "                            transforms.Resize(256),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            normalize,\n",
    "                        ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_data = Path('datasets', 'monkey_pose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in datasets/monkey_pose/train_data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matthias/Storage/Monkey/monkey-pose-classification/pose_prediction.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matthias/Storage/Monkey/monkey-pose-classification/pose_prediction.ipynb#ch0000006?line=0'>1</a>\u001b[0m datasets_images \u001b[39m=\u001b[39m {x: datasets\u001b[39m.\u001b[39mImageFolder((directory_data \u001b[39m/\u001b[39m x), transforming_data[x])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthias/Storage/Monkey/monkey-pose-classification/pose_prediction.ipynb#ch0000006?line=1'>2</a>\u001b[0m                         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain_data\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalidation_data\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthias/Storage/Monkey/monkey-pose-classification/pose_prediction.ipynb#ch0000006?line=2'>3</a>\u001b[0m                     }\n",
      "\u001b[1;32m/home/matthias/Storage/Monkey/monkey-pose-classification/pose_prediction.ipynb Cell 8'\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matthias/Storage/Monkey/monkey-pose-classification/pose_prediction.ipynb#ch0000006?line=0'>1</a>\u001b[0m datasets_images \u001b[39m=\u001b[39m {x: datasets\u001b[39m.\u001b[39;49mImageFolder((directory_data \u001b[39m/\u001b[39;49m x), transforming_data[x])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthias/Storage/Monkey/monkey-pose-classification/pose_prediction.ipynb#ch0000006?line=1'>2</a>\u001b[0m                         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain_data\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalidation_data\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthias/Storage/Monkey/monkey-pose-classification/pose_prediction.ipynb#ch0000006?line=2'>3</a>\u001b[0m                     }\n",
      "File \u001b[0;32m~/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=301'>302</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=302'>303</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=303'>304</a>\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=307'>308</a>\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=308'>309</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=309'>310</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=310'>311</a>\u001b[0m         root,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=311'>312</a>\u001b[0m         loader,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=312'>313</a>\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=313'>314</a>\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=314'>315</a>\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=315'>316</a>\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=316'>317</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=317'>318</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m~/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=134'>135</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=135'>136</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=136'>137</a>\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=141'>142</a>\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=142'>143</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=143'>144</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=144'>145</a>\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=145'>146</a>\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=147'>148</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[0;32m~/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py:219\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=191'>192</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=192'>193</a>\u001b[0m     \u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=193'>194</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=194'>195</a>\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=216'>217</a>\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=217'>218</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=218'>219</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=40'>41</a>\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[0;32m---> <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=42'>43</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=44'>45</a>\u001b[0m class_to_idx \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n\u001b[1;32m     <a href='file:///home/matthias/Programming/miniconda3/envs/mpc/lib/python3.8/site-packages/torchvision/datasets/folder.py?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in datasets/monkey_pose/train_data."
     ]
    }
   ],
   "source": [
    "datasets_images = {x: datasets.ImageFolder((directory_data / x), transforming_data[x])\n",
    "                        for x in ['train_data', 'validation_data']\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders_data = {x: torch.utils.data.DataLoader(datasets_images[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "                    for x in ['train_data', 'validation_data']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_datasets = {x: len(datasets_images[x]) for x in ['train_data', 'validation_data']}\n",
    "\n",
    "class_names = datasets_images['train_data'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data *optional*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()   # This is the interactive mode\n",
    "def visualize_data(input, title=None):\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    plt.imshow(input)\n",
    "    if title is not None:\n",
    "       plt.title(title)\n",
    "    plt.pause(0.001)  ## Here we are pausing a bit so that plots are updated\n",
    "inputs_data, classes = next(iter(loaders_data['train_data']))\n",
    "## This is the code for getting a batch of training data\n",
    "out = torchvision.utils.make_grid(inputs_data)\n",
    "## Here we are making a grid from batch\n",
    "visualize_data(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(res_model, criterion, optimizer, scheduler, number_epochs=25):\n",
    "    since = time.time()\n",
    "    best_resmodel_wts = copy.deepcopy(res_model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "    for epochs in range(number_epochs):\n",
    "        print('Epoch {}/{}'.format(epochs, number_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        for phase in ['train_data', 'validation_data']: ## Here each epoch is having a training and validation phase\n",
    "            if phase == 'train_data':\n",
    "               res_model.train()  ## Here we are setting our model to training mode\n",
    "            else:\n",
    "               res_model.eval()   ## Here we are setting our model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in loaders_data[phase]: ## Iterating over data.\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad() ## here we are making the gradients to zero\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train_data'): ## forwarding and then tracking the history if only in train\n",
    "                     outputs = res_model(inputs)\n",
    "                     _, preds = torch.max(outputs, 1)\n",
    "                     loss = criterion(outputs, labels)\n",
    "\n",
    "                     if phase == 'train': # backward and then optimizing only if it is in training phase\n",
    "                         loss.backward()\n",
    "                         optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "             epoch_loss = running_loss / sizes_datasets[phase]\n",
    "             epoch_acc = running_corrects.double() / sizes_datasets[phase]\n",
    "\n",
    "             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "             if phase == 'val' and epoch_acc > best_acc: ## deep copy the model\n",
    "                 best_accuracy = epoch_acc\n",
    "                 best_resmodel_wts = copy.deepcopy(res_model.state_dict())\n",
    "\n",
    "         print()\n",
    "\n",
    "     time_elapsed = time.time() - since\n",
    "     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "     print('Best val Acc: {:4f}'.format(best_accuracy))\n",
    "\n",
    "     # load best model weights\n",
    "     res_model.load_state_dict(best_resmodel_wts)\n",
    "     return res_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_visualization(res_model, num_images=6):\n",
    "    was_training = res_model.training\n",
    "    res_model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(loaders_data['validation_data']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = res_model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "            visualize_data(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "               res_model.train(mode=was_training)\n",
    "               return\n",
    "        res_model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Convet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = models.resnet50(pretrained=True)\n",
    "num_ftrs = finetune_model.fc.in_features\n",
    "\n",
    "finetune_model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "finetune_model = finetune_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "finetune_optim = optim.SGD(finetune_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = model_training(finetune_model, criterion, finetune_optim, exp_lr_scheduler,\n",
    "                       number_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_visualization(finetune_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46e961110a7e0cb6a866fceb9656f583b8efaaf44d1cf4597e75b5ea0fa4ace1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mpc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
